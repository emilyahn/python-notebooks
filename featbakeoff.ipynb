{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Design Bakeoff: Native Language Identification\n",
    "\n",
    "## The Task\n",
    "\n",
    "Given an essay written in English by a non-native speaker, identify their native language. This is a task published by ETS (Educational Testing Service), with the data coming from TOEFL essays.\n",
    "\n",
    "**Data:** Files for training, development, and test, automatically downloaded and parsed by the code below. The test data does not have labels. The languages in the data are \n",
    "\n",
    "    ARA = Arabic\n",
    "    CHI = Chinese\n",
    "    FRE = French\n",
    "    GER = German\n",
    "    HIN = Hindi\n",
    "    ITA = Italian\n",
    "    JPN = Japanese\n",
    "    KOR = Korean\n",
    "    SPA = Spanish\n",
    "    TEL = Telugu\n",
    "    TUR = Turkish\n",
    "\n",
    "**Learning Algorithm:** One-versus-rest logistic regression\n",
    "\n",
    "**Features:** You design 'em! A baseline with word counts is given to you.\n",
    "\n",
    "## 1. Load Data\n",
    "\n",
    "(This will take a few seconds since it has to read the data over the Internet.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "def load_data(url):\n",
    "    \"\"\"read a data file from the web\"\"\"\n",
    "    obj = urllib2.urlopen(url)\n",
    "    return [line.strip() for line in obj.readlines()]\n",
    "\n",
    "train_text = load_data('http://cs.wellesley.edu/~sravana/ml/nli/training.essays')\n",
    "train_labels = load_data('http://cs.wellesley.edu/~sravana/ml/nli/training.labels')\n",
    "\n",
    "dev_text = load_data('http://cs.wellesley.edu/~sravana/ml/nli/development.essays')\n",
    "dev_labels = load_data('http://cs.wellesley.edu/~sravana/ml/nli/development.labels')\n",
    "\n",
    "test_text = load_data('http://cs.wellesley.edu/~sravana/ml/nli/test.essays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview data\n",
    "\n",
    "Here are what the essays look like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'There are', len(train_text), 'essays in the training set'\n",
    "print 'A sample essay by a speaker of', train_labels[0]\n",
    "print train_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Featurizing and Classification\n",
    "\n",
    "The code below transforms the language labels ('ARA', 'CHI', etc) into integer labels for the training and development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "trainy = le.fit_transform(train_labels)  \n",
    "devy = le.transform(dev_labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline featurizer with bag-of-word counts is implemented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()  # initialize object with CountVectorizer defaults\n",
    "\n",
    "# convert to array where each row is an essay, each dimension is a word, \n",
    "# and each value is the count of that word in the essay\n",
    "trainX = vectorizer.fit_transform(train_text)  \n",
    "\n",
    "trainX = trainX.toarray()   # make dense\n",
    "\n",
    "devX = vectorizer.transform(dev_text)  # featurize the development text\n",
    "devX = devX.toarray()\n",
    "\n",
    "testX = vectorizer.transform(test_text)  # featurize the testing text\n",
    "testX = testX.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a multi-class logistic regression model and test on the dev set. This code may take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()  # one-versus rest logistic regression\n",
    "\n",
    "model.fit(trainX, trainy)\n",
    "accuracy = model.score(devX, devy)\n",
    "print 'Classification accuracy', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different options for the `CountVectorizer` (which allows word and character n-grams) or replace it with the `TfidfVectorizer` and explore *its* different options. See [the documentation on text feature extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text).\n",
    "\n",
    "You can also experiment with some of sklearn's [feature scaling and normalization options](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).\n",
    "\n",
    "In addition to feature engineering, you can also try alternate hyperparameters for the logistic regression model above.\n",
    "\n",
    "If you're really efficient, you can concatenate some hand-designed features (average word length, etc) to the trainX, testX, and devX arrays. This takes some time to implement, so leave it to the end.\n",
    "\n",
    "When you're satisfied with the development accuracy, predict labels for test data by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(testX)\n",
    "predictions = le.inverse_transform(predictions)  # transform list of indices into list of languages\n",
    "\n",
    "team_name = raw_input('Enter your team name: ').strip().replace(' ', '_')\n",
    "student_names = raw_input('Enter all student names, separated by commas: ').strip()\n",
    "\n",
    "with open(team_name+'.results', 'w') as o:\n",
    "    o.write(student_names+'\\n')\n",
    "    o.write('\\n'.join(predictions))\n",
    "print 'Wrote results to file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `<your_team_name>.results` file that was created in the current working directory into\n",
    "[this Google Drive folder](https://drive.google.com/open?id=0B8FnZZJ_NRjiMXBkN2YtMDF2dzg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
